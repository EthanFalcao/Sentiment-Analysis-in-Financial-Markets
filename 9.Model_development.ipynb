{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/df_fs.csv', sep=\",\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Open</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>30_day_MA</th>\n",
       "      <th>60_day_MA</th>\n",
       "      <th>90_day_MA</th>\n",
       "      <th>RSI</th>\n",
       "      <th>MACD</th>\n",
       "      <th>MACD_Histogram</th>\n",
       "      <th>Price_Change_Pct</th>\n",
       "      <th>Article Length</th>\n",
       "      <th>article_sentiment</th>\n",
       "      <th>Price_Change_Category</th>\n",
       "      <th>smoothed_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-07-20</td>\n",
       "      <td>56.84</td>\n",
       "      <td>55.53</td>\n",
       "      <td>55.91</td>\n",
       "      <td>89893300.00</td>\n",
       "      <td>51.95</td>\n",
       "      <td>52.31</td>\n",
       "      <td>50.52</td>\n",
       "      <td>71.72</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.52</td>\n",
       "      <td>5.31</td>\n",
       "      <td>1394.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>positive</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-07-25</td>\n",
       "      <td>56.74</td>\n",
       "      <td>56.26</td>\n",
       "      <td>56.73</td>\n",
       "      <td>25610600.00</td>\n",
       "      <td>52.10</td>\n",
       "      <td>52.40</td>\n",
       "      <td>50.66</td>\n",
       "      <td>74.61</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.28</td>\n",
       "      <td>623.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>positive</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>56.75</td>\n",
       "      <td>56.14</td>\n",
       "      <td>56.58</td>\n",
       "      <td>26003400.00</td>\n",
       "      <td>52.25</td>\n",
       "      <td>52.48</td>\n",
       "      <td>50.80</td>\n",
       "      <td>91.39</td>\n",
       "      <td>1.03</td>\n",
       "      <td>0.67</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>269.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>negative</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-08-03</td>\n",
       "      <td>57.11</td>\n",
       "      <td>56.49</td>\n",
       "      <td>56.97</td>\n",
       "      <td>22075600.00</td>\n",
       "      <td>52.41</td>\n",
       "      <td>52.55</td>\n",
       "      <td>50.95</td>\n",
       "      <td>91.80</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.69</td>\n",
       "      <td>904.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>positive</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-08-05</td>\n",
       "      <td>58.21</td>\n",
       "      <td>57.45</td>\n",
       "      <td>57.96</td>\n",
       "      <td>29335200.00</td>\n",
       "      <td>52.61</td>\n",
       "      <td>52.66</td>\n",
       "      <td>51.12</td>\n",
       "      <td>92.48</td>\n",
       "      <td>1.40</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.99</td>\n",
       "      <td>679.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>positive</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  Open   Low  Close      Volume  30_day_MA  60_day_MA  90_day_MA  \\\n",
       "0  2016-07-20 56.84 55.53  55.91 89893300.00      51.95      52.31      50.52   \n",
       "1  2016-07-25 56.74 56.26  56.73 25610600.00      52.10      52.40      50.66   \n",
       "2  2016-08-01 56.75 56.14  56.58 26003400.00      52.25      52.48      50.80   \n",
       "3  2016-08-03 57.11 56.49  56.97 22075600.00      52.41      52.55      50.95   \n",
       "4  2016-08-05 58.21 57.45  57.96 29335200.00      52.61      52.66      51.12   \n",
       "\n",
       "    RSI  MACD  MACD_Histogram  Price_Change_Pct  Article Length  \\\n",
       "0 71.72  0.56            0.52              5.31         1394.00   \n",
       "1 74.61  0.84            0.64              0.28          623.00   \n",
       "2 91.39  1.03            0.67             -0.18          269.00   \n",
       "3 91.80  1.20            0.67              0.69          904.00   \n",
       "4 92.48  1.40            0.70              0.99          679.00   \n",
       "\n",
       "   article_sentiment Price_Change_Category  smoothed_sentiment  \n",
       "0               4.00              positive                4.00  \n",
       "1               4.00              positive                4.00  \n",
       "2               4.00              negative                4.00  \n",
       "3               4.00              positive                4.00  \n",
       "4               1.00              positive                3.40  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4983 entries, 0 to 4982\n",
      "Data columns (total 16 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   date                   4983 non-null   object \n",
      " 1   Open                   4983 non-null   float64\n",
      " 2   Low                    4983 non-null   float64\n",
      " 3   Close                  4983 non-null   float64\n",
      " 4   Volume                 4983 non-null   float64\n",
      " 5   30_day_MA              4983 non-null   float64\n",
      " 6   60_day_MA              4983 non-null   float64\n",
      " 7   90_day_MA              4983 non-null   float64\n",
      " 8   RSI                    4983 non-null   float64\n",
      " 9   MACD                   4983 non-null   float64\n",
      " 10  MACD_Histogram         4983 non-null   float64\n",
      " 11  Price_Change_Pct       4983 non-null   float64\n",
      " 12  Article Length         4983 non-null   float64\n",
      " 13  article_sentiment      4983 non-null   float64\n",
      " 14  Price_Change_Category  4983 non-null   object \n",
      " 15  smoothed_sentiment     4983 non-null   float64\n",
      "dtypes: float64(14), object(2)\n",
      "memory usage: 661.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Mean Absolute Error (MAE): 0.7903134051632317\n",
      "Mean Squared Error (MSE): 1.6012902408812697\n",
      "R-squared (R2): 0.6442192078734159\n",
      "\n",
      "Linear Regression Summary:\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:       Price_Change_Pct   R-squared:                       0.689\n",
      "Model:                            OLS   Adj. R-squared:                  0.688\n",
      "Method:                 Least Squares   F-statistic:                     1102.\n",
      "Date:                Thu, 25 Apr 2024   Prob (F-statistic):               0.00\n",
      "Time:                        11:41:33   Log-Likelihood:                -6022.3\n",
      "No. Observations:                3986   AIC:                         1.206e+04\n",
      "Df Residuals:                    3977   BIC:                         1.212e+04\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.1751      0.017     10.071      0.000       0.141       0.209\n",
      "x1           -13.3928      0.661    -20.260      0.000     -14.689     -12.097\n",
      "x2           -17.8970      0.595    -30.084      0.000     -19.063     -16.731\n",
      "x3            48.7966      0.769     63.493      0.000      47.290      50.303\n",
      "x4           -17.6196      0.564    -31.220      0.000     -18.726     -16.513\n",
      "x5             0.1723      0.026      6.505      0.000       0.120       0.224\n",
      "x6            -1.6484      0.059    -27.801      0.000      -1.765      -1.532\n",
      "x7             0.5967      0.023     25.531      0.000       0.551       0.643\n",
      "x8             0.0073      0.017      0.420      0.674      -0.027       0.042\n",
      "==============================================================================\n",
      "Omnibus:                      725.576   Durbin-Watson:                   2.017\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            10956.591\n",
      "Skew:                           0.413   Prob(JB):                         0.00\n",
      "Kurtosis:                      11.080   Cond. No.                         103.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Define relevant features and the target variable\n",
    "\n",
    "#additional_features = [\"MACD_Histogram\",\"RSI\",\"MACD\", 'EMA_12',\"Article Length\" ]\n",
    "#additional_features = ['RSI', 'Volume', 'MACD_Histogram', 'Timestamp', 'Day', 'Signal_Line', 'Month', 'DayOfWeek', 'Close', 'Article Length', 'Open', 'Low']\n",
    "#X = df[['article_sentiment'] + additional_features]  # Include 'article_sentiment' and other features\n",
    "\n",
    "#X = df.drop([\"Price_Change_Pct\", \"date\",\"Price_Change_Category\"], axis=1)  # Exclude target variable and date\n",
    "X = df[['Open', 'Low', 'Close', '60_day_MA', 'RSI', 'MACD', 'MACD_Histogram', 'article_sentiment']]\n",
    "y = df['Price_Change_Pct']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create a linear regression model\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "lin_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = lin_reg.predict(X_test_scaled)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Output model coefficients and performance metrics\n",
    "coefficients = dict(zip(X.columns, lin_reg.coef_))  # Get model coefficients\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"R-squared (R2):\", r2)\n",
    "\n",
    "# Add a constant term to the features\n",
    "X_train_with_const = sm.add_constant(X_train_scaled)\n",
    "\n",
    "# Create and fit the model\n",
    "model = sm.OLS(y_train, X_train_with_const)\n",
    "results = model.fit()\n",
    "\n",
    "# Print the summary\n",
    "print(\"\\nLinear Regression Summary:\")\n",
    "print(results.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBRegressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Mean Absolute Error (MAE): 0.5094086234515847\n",
      "Mean Squared Error (MSE): 0.7527561996655483\n",
      "R-squared (R2): 0.8327497475736734\n",
      "\n",
      "Feature Importance:\n",
      "{'f0': 854.0, 'f1': 437.0, 'f2': 483.0, 'f3': 421.0, 'f4': 720.0, 'f5': 677.0, 'f6': 621.0, 'f7': 122.0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor  # Import XGBRegressor\n",
    "\n",
    "# Define features and the target variable\n",
    "X = df[['Open', 'Low', 'Close', '60_day_MA', 'RSI', 'MACD', 'MACD_Histogram', 'article_sentiment']]\n",
    "y = df['Price_Change_Pct']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Standardize the training set\n",
    "X_test_scaled = scaler.transform(X_test)  # Standardize the testing set\n",
    "\n",
    "# Create the XGBRegressor model\n",
    "xgb_reg = XGBRegressor(objective='reg:squarederror', random_state=42)  # 'reg:squarederror' for regression tasks\n",
    "\n",
    "# Fit the model to the training data\n",
    "xgb_reg.fit(X_train_scaled, y_train)  # Ensure the model is trained\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = xgb_reg.predict(X_test_scaled)  # Predict on the test set\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)  # Mean Absolute Error\n",
    "mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error\n",
    "r2 = r2_score(y_test, y_pred)  # R-squared\n",
    "\n",
    "# Retrieve feature importance using the correct method\n",
    "feature_importance = xgb_reg.get_booster().get_score(importance_type='weight')  # Get feature importance by weight\n",
    "\n",
    "# Output model coefficients and performance metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"R-squared (R2):\", r2)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance)  # Display feature importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "f0: 854.0\n",
      "f1: 437.0\n",
      "f2: 483.0\n",
      "f3: 421.0\n",
      "f4: 720.0\n",
      "f5: 677.0\n",
      "f6: 621.0\n",
      "f7: 122.0\n"
     ]
    }
   ],
   "source": [
    "# Retrieve feature importance from the XGBoost model\n",
    "feature_importance = xgb_reg.get_booster().get_score(importance_type='weight')\n",
    "\n",
    "# Display the feature importance summary\n",
    "print(\"Feature Importance:\")\n",
    "for feature, importance in feature_importance.items():\n",
    "    print(f\"{feature}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lazypredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 10/42 [00:04<00:14,  2.15it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m clf \u001b[38;5;241m=\u001b[39m LazyRegressor(verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,predictions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, ignore_warnings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, custom_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m----> 8\u001b[0m models,predictions \u001b[38;5;241m=\u001b[39m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(models)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\lazypredict\\Supervised.py:603\u001b[0m, in \u001b[0;36mLazyRegressor.fit\u001b[1;34m(self, X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    599\u001b[0m     pipe \u001b[38;5;241m=\u001b[39m Pipeline(\n\u001b[0;32m    600\u001b[0m         steps\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m, preprocessor), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregressor\u001b[39m\u001b[38;5;124m\"\u001b[39m, model())]\n\u001b[0;32m    601\u001b[0m     )\n\u001b[1;32m--> 603\u001b[0m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels[name] \u001b[38;5;241m=\u001b[39m pipe\n\u001b[0;32m    605\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\pipeline.py:427\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    426\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 427\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_last_step)\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\gaussian_process\\_gpr.py:335\u001b[0m, in \u001b[0;36mGaussianProcessRegressor.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_marginal_likelihood_value_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mmin(lml_values)\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_marginal_likelihood_value_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_marginal_likelihood\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclone_kernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;66;03m# Precompute quantities required for predictions which are independent\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;66;03m# of actual query points\u001b[39;00m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Alg. 2.1, page 19, line 2 -> L = cholesky(K + sigma^2 I)\u001b[39;00m\n\u001b[0;32m    342\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train_)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\gaussian_process\\_gpr.py:587\u001b[0m, in \u001b[0;36mGaussianProcessRegressor.log_marginal_likelihood\u001b[1;34m(self, theta, eval_gradient, clone_kernel)\u001b[0m\n\u001b[0;32m    585\u001b[0m K[np\u001b[38;5;241m.\u001b[39mdiag_indices_from(K)] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha\n\u001b[0;32m    586\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 587\u001b[0m     L \u001b[38;5;241m=\u001b[39m \u001b[43mcholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGPR_CHOLESKY_LOWER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mLinAlgError:\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39mzeros_like(theta)) \u001b[38;5;28;01mif\u001b[39;00m eval_gradient \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\linalg\\_decomp_cholesky.py:89\u001b[0m, in \u001b[0;36mcholesky\u001b[1;34m(a, lower, overwrite_a, check_finite)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcholesky\u001b[39m(a, lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, overwrite_a\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, check_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     46\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m    Compute the Cholesky decomposition of a matrix.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     87\u001b[0m \n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m     c, lower \u001b[38;5;241m=\u001b[39m \u001b[43m_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_finite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\scipy\\linalg\\_decomp_cholesky.py:35\u001b[0m, in \u001b[0;36m_cholesky\u001b[1;34m(a, lower, overwrite_a, clean, check_finite)\u001b[0m\n\u001b[0;32m     33\u001b[0m overwrite_a \u001b[38;5;241m=\u001b[39m overwrite_a \u001b[38;5;129;01mor\u001b[39;00m _datacopied(a1, a)\n\u001b[0;32m     34\u001b[0m potrf, \u001b[38;5;241m=\u001b[39m get_lapack_funcs((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpotrf\u001b[39m\u001b[38;5;124m'\u001b[39m,), (a1,))\n\u001b[1;32m---> 35\u001b[0m c, info \u001b[38;5;241m=\u001b[39m \u001b[43mpotrf\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-th leading minor of the array is not positive \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     38\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefinite\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m info)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import lazypredict\n",
    "from lazypredict.Supervised import LazyRegressor\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "clf = LazyRegressor(verbose=0,predictions=True, ignore_warnings=True, custom_metric=None)\n",
    "models,predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
